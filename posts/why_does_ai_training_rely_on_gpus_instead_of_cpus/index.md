# 为什么大模型训练依赖 GPU 而不是 CPU


![img.png](./images/img.png)

最近 AI 大模型的热度可谓是如火如荼，尤其是国内的 DeepSeek 名闻遐迩，因其与 OpenAI 的模型相比，DeepSeek 推理大模型 R1 的训练成本仅为其 3%-5%‌ ，直接导致英伟达公司股票短时间内大跌。

此时我就比较好奇，为什么大模型训练中更多的是依赖 GPU，而不是 CPU 呢？

随后这两天网上翻阅了些资料，也询问 AI 帮忙解答，大体总结出如下几点：
<br></br>

**一、并行计算能力**

GPU 具有高度并行的架构，能够同时处理大量的数据和计算任务。比如在进行矩阵运算时，它可以同时对多个元素进行操作，瞬间完成复杂的计算。

相比之下，CPU 虽然在串行计算方面表现不错，但并行处理能力相对较弱，无法像 GPU 那样同时处理众多任务。
<br></br>

**二、内存带宽**

大模型训练需要处理的数据量巨大，比如 `DeepSeek R1` 满血版参数就高达 **671B** （6710亿）这对内存带宽要求极高。

GPU 配备了超高的内存带宽，就类似拥有一条宽阔的高速公路，数据可以快速地传输到计算核心进行处理。

而 CPU 的内存带宽则相对有限，面对海量数据时，就像走在狭窄的小道上，容易形成数据传输的瓶颈，拖慢训练速度。
<br></br>

**三、浮点运算性能**

在大模型训练中，离不开浮点运算。

GPU 在浮点运算性能方面优势明显，它拥有大量的浮点运算单元，能够在单位时间内执行海量的浮点运算操作。

CPU 的浮点运算能力相对较弱，在处理大规模的浮点运算时，效率远远比不上 GPU 。
<br></br>

**四、深度学习框架支持**

现在主流的深度学习框架，像是 TensorFlow 和 PyTorch 等，都对 GPU 进行了深度优化和兼容，能够充分发挥其强大的性能，自动将计算任务合理分配到 GPU 上并行执行。

然而，对于 CPU 的优化就没那么给力，导致在实际训练中，使用 CPU 的效果不尽如人意。
<br></br>

**五、能耗效率**

常理来看 GPU 功耗好像挺高，但从单位时间完成的计算量来看，它的能耗效率还是可以的。

虽然 CPU 功耗低，但完成相同的训练任务需要更长时间，综合能耗并不占优势。
<br></br>

**六、成本效益**

GPU 单价相比 CPU 确实较高，但是它能大幅缩短训练时间。有句话说的好，时间就是金钱！这样算下来，总体的成本反而更低。

而 CPU 训练时间长，资源占用和维护成本累加起来也不少。

所以，综合以上这些方面看，GPU 在大模型训练中脱颖而出，而 CPU 则在这个领域就有点力不从心了。


---

> 作者: [Vespeng](https://github.com/vespeng/)  
> URL: https://vespeng.tech/posts/why_does_ai_training_rely_on_gpus_instead_of_cpus/  

